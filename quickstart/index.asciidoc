---
layout: base
---

QuickStart into GlusterFS
-------------------------

This document is intended to give you a step by step guide to setting up Gluster for the first time.  For this tutorial, we will assume you are using Fedora virtual machines (other distributions and methods can be found in the new user guide, below.  We also do not explain the steps in detail here, this guide is just to help you get it up and running as soon as possible.  After you deploy Gluster by following these steps, we recommend that you read the Gluster Admin Guide TODO:link! to learn how to administer Gluster and how to select a volume type that fits your needs.  Read the Gluster New User Guide TODO:link! for a more detailed explanation of the steps we took here.  We want you to be successful in as short a time as possible.

If you would like a more detailed walk through with instructions for installing using different methods (in local virtual machines, EC2 and baremetal) and different distributions, then have a look at the _Getting Started with Gluster_ TODO:link! guide.  

Really Really Quick Start Guide
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want the bare minumum steps to getting started with Gluster, take a look at the _Really Really Quick Start Guide_ TODO:link!.

Step 1 –  Have at least two nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 * A 64 bit OS
 * A working network connection
 * At least two virtual disks, one for the OS installation, and one to be used 
   to serve Gluster storage.  This will emulate a real world deployment, where 
   you would want seperate disks for Gluster storage than the OS install.

Step 2 - Format and mount the bricks on both nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Note:* This step assumes you have already partitioned the disks.

[source]
   mkfs.xfs -i size=512 /dev/sdb1


Step 3 - Add an entry to /etc/fstab
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[source]
(on both nodes)
   mkdir -p /export/brick1
   vi /etc/fstab
Add the following:
   /dev/sdb1 /export/brick1 xfs defaults 1 2
Save the file and exit
   mount -a && mount

You should now see the Gluster export directory mounted on /brick1

Now create a subdir in the mountpoint.

[source]
mkdir /export/brick1/sdb1

Step 4 – Installing Gluster on both servers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Install the yum repo

CentOS/RHEL:
[source]
   wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo

Fedora:
[source]
   wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/Fedora/glusterfs-fedora.repo

Then install the software
[source]
   yum install glusterfs{-fuse,-server}

Start the Gluster service:
[source]
   service glusterd start
   service glusterd status
   glusterd.service - LSB: glusterfs server
    	  Loaded: loaded (/etc/rc.d/init.d/glusterd)
   	  Active: active (running) since Mon, 13 Aug 2012 13:02:11 -0700; 2s ago
   	 Process: 19254 ExecStart=/etc/rc.d/init.d/glusterd start (code=exited, status=0/SUCCESS)
   	  CGroup: name=systemd:/system/glusterd.service
   		  ├ 19260 /usr/sbin/glusterd -p /run/glusterd.pid
   		  ├ 19304 /usr/sbin/glusterfsd --xlator-option georep-server.listen-port=24009 -s localhost...
   		  └ 19309 /usr/sbin/glusterfs -f /var/lib/glusterd/nfs/nfs-server.vol -p /var/lib/glusterd/...

Step 5 – Configure the trusted pool
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[source]
   gluster peer probe <ip or hostname of other node>

*Note:* When using hostnames, the first server needs to be probed from one other server to set it's hostname.

Step 6 – Set up a Gluster volume
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

From a single node:

[source]
   gluster volume create gv0 replica 2 node01.mydomain.net:/export/brick1/sdb1 n   ode02.mydomain.net:/export/brick1/sdb1
   gluster volume start gv0

Confirm that the volume shows "Started":

[source]
   gluster volume info

Step 7 – Testing the Gluster volume
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For this step, we will use one of the Gluster servers to mount the
volume.  Typically, you would do this from an external machine, known
as a "client".  Since using the method here would require additional
packages be installed on the client machine, we will use the servers
as a simple place to test first.

[source]
   mount -t glusterfs node01.yourdomain.net:/gv0 /mnt
   for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done

First, check the mount point:

[source]
   ls -lA /mnt | wc -l

You should see 100 files returned. Next, check the Gluster mount points on each server:

[source]
   ls -lA /export/brick1

You should see 100 per server using the method we listed here.  Without replication, in a distribute only volume (not detailed here), you should see about 50.

